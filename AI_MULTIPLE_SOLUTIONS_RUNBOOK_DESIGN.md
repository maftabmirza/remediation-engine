# AI Helper: Multiple Solutions & Runbook-First Integration Design

**Version:** 1.0
**Date:** 2026-01-05
**Status:** High-Level Design (Pre-Implementation)

---

## Executive Summary

This document outlines the high-level design for enhancing the AI Helper to:
1. **Present multiple solution options** instead of suggesting a single command/action
2. **Prioritize runbook execution** when automated runbooks exist for the identified problem
3. **Let users choose** the most appropriate solution based on their context and preferences

This design integrates with the existing troubleshooting storage and feedback system (Option C: Hybrid feedback model).

---

## Problem Statement

### Current Limitations
- AI Helper currently suggests **single commands to execute** without presenting alternatives
- No integration with existing **runbook system** for automated remediation
- Users have no choice when **multiple valid solutions** exist for the same problem
- No consideration of **user preferences** (automated vs manual, quick fix vs thorough investigation)

### Requirements
- Present **2-3 solution options** when multiple approaches are available
- **Runbook-first approach**: Always check for existing runbooks before suggesting manual commands
- **RBAC/ACL enforcement**: Respect runbook execution permissions
- **User choice tracking**: Learn which solutions users prefer over time
- **Context-aware ranking**: Match solutions to specific server types, applications, environments

---

## Design Overview

### Core Principles

1. **Runbook-First Strategy**
   - Always search for matching runbooks in parallel with manual solutions
   - Prioritize automated solutions over manual intervention
   - Suggest creating runbooks for frequently-used manual solutions

2. **Multiple Options Presentation**
   - Show 2-3 ranked options when confidence scores are similar
   - Present clear trade-offs (time, risk, automation level, success rate)
   - Let users make informed decisions

3. **Smart Decision Matrix**
   - Auto-select when one solution is clearly superior (confidence > 0.9)
   - Present options when confidence differences are small (< 0.1)
   - Mark solutions as "experimental" when no high-confidence match exists

4. **User-Centric Design**
   - Track which solutions users choose
   - Learn user preferences over time
   - Provide transparency in decision-making

---

## Solution Architecture

### 1. Workflow Integration

```
User Query ‚Üí AI Helper Orchestrator
    ‚Üì
    ‚îú‚îÄ‚Üí Parallel Search:
    ‚îÇ   ‚îú‚îÄ‚Üí Knowledge Base Search (Design docs, best practices)
    ‚îÇ   ‚îú‚îÄ‚Üí Troubleshooting History Search (Past solutions)
    ‚îÇ   ‚îî‚îÄ‚Üí Runbook Search (Automated remediation)
    ‚Üì
Solution Ranker & Combiner
    ‚Üì
Decision Matrix (Determine presentation strategy)
    ‚Üì
Unified Response Generation
    ‚Üì
User Selection ‚Üí Feedback Loop
```

### 2. Decision Matrix

```
IF confidence difference < 0.1 (solutions very similar):
   ‚Üí Show multiple options (2-3 max)
   ‚Üí Let user choose
   ‚Üí Track selection for learning

ELSE IF top result confidence > 0.9:
   ‚Üí Show top result as primary recommendation
   ‚Üí Provide "See alternatives" link
   ‚Üí User can expand to see other options

ELSE IF no high-confidence result (all < 0.7):
   ‚Üí Show top 2-3 options
   ‚Üí Mark as "experimental" or "low confidence"
   ‚Üí Request user feedback after execution

ELSE (one clear winner, confidence 0.7-0.9):
   ‚Üí Show primary + one alternative
   ‚Üí Explain why primary is recommended
```

### 3. Runbook-First Logic

```
1. Query Received (e.g., "High CPU on Apache server")
   ‚Üì
2. Parallel Search:
   - Runbook semantic search: "apache cpu high utilization"
   - Manual solution search: Past troubleshooting cases
   ‚Üì
3. Context Filtering:
   - Server type: Apache web server
   - Application: PHP application
   - OS: Ubuntu 20.04
   - Environment: Production
   ‚Üì
4. Runbook Match Found?
   YES ‚Üí Prioritize runbook
   NO ‚Üí Show manual solutions only
   ‚Üì
5. Runbook RBAC Check:
   - Can user execute this runbook?
   - Does user have required permissions?
   ‚Üì
6. Confidence Comparison:
   - Runbook confidence: 0.95
   - Top manual solution confidence: 0.85
   ‚Üí Show runbook as primary, manual as alternative
```

---

## Unified Response Structure

### Response Format

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üéØ RECOMMENDED SOLUTIONS                                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ [AUTOMATED RUNBOOK] (If available)                            ‚îÇ
‚îÇ ‚úÖ Runbook #45: "Apache High CPU - Graceful Restart"         ‚îÇ
‚îÇ    ‚≠ê‚≠ê‚≠ê Confidence: 95%                                     ‚îÇ
‚îÇ    ‚è±Ô∏è  Estimated time: 5 minutes                             ‚îÇ
‚îÇ    üìä Success rate: 100% (45/45 executions)                  ‚îÇ
‚îÇ    üîí RBAC: You have permission to execute                   ‚îÇ
‚îÇ    üìù Description: Gracefully restarts Apache with traffic   ‚îÇ
‚îÇ        drain and health check validation                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ    [‚ñ∂Ô∏è  Execute Runbook] [üìñ View Details]                   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ [ALTERNATIVE MANUAL SOLUTIONS]                                ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ 1. Restart Apache Service (Manual)                           ‚îÇ
‚îÇ    ‚≠ê‚≠ê Confidence: 85%                                       ‚îÇ
‚îÇ    ‚è±Ô∏è  Estimated time: 3 minutes                             ‚îÇ
‚îÇ    üìä Success rate: 90% (18/20 cases)                        ‚îÇ
‚îÇ    üìù Commands:                                               ‚îÇ
‚îÇ        sudo systemctl restart apache2                        ‚îÇ
‚îÇ        sudo systemctl status apache2                         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ    [üìã Copy Commands] [üëÅÔ∏è  Show Details]                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ 2. Optimize Apache Memory Configuration                      ‚îÇ
‚îÇ    ‚≠ê Confidence: 80%                                         ‚îÇ
‚îÇ    ‚è±Ô∏è  Estimated time: 15 minutes                            ‚îÇ
‚îÇ    üìä Success rate: 80% (12/15 cases)                        ‚îÇ
‚îÇ    üìù Approach: Tune MaxRequestWorkers and memory limits     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ    [üìã Show Steps] [üëÅÔ∏è  Show Details]                        ‚îÇ
‚îÇ                                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ [KNOWLEDGE BASE REFERENCE]                                    ‚îÇ
‚îÇ üìö Related Documentation:                                     ‚îÇ
‚îÇ    - Apache Performance Tuning Guide (grafana-docs)          ‚îÇ
‚îÇ    - Production Server Best Practices                        ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ    [üîó View Docs]                                             ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üí° Recommendation: The automated runbook is the safest option with
   proven success. It handles traffic gracefully and validates health.

‚ö†Ô∏è  Note: All manual commands require sudo privileges.
```

---

## Component Design

### 1. Runbook Search Service

**Purpose:** Semantic search across existing runbooks to find automated solutions

**Inputs:**
- User query (natural language)
- Context metadata (server_type, app, OS, environment)
- User permissions (RBAC roles)

**Outputs:**
- Ranked list of matching runbooks
- Confidence scores (semantic similarity)
- Execution metadata (success rate, duration, last run)
- RBAC eligibility (can user execute?)

**Search Strategy:**
```
1. Generate query embedding (same model as troubleshooting)
2. Semantic search in runbook embeddings (pgvector)
3. Apply context filters:
   - server_type MATCH
   - application MATCH
   - environment MATCH (prod/staging/dev)
   - OS compatibility
4. Apply RBAC filter:
   - Remove runbooks user cannot execute
5. Rank by:
   - Semantic similarity (0.5 weight)
   - Success rate (0.3 weight)
   - Context match quality (0.2 weight)
6. Return top 3 runbooks
```

### 2. Solution Ranker & Combiner

**Purpose:** Combine runbook + manual solutions and rank by confidence, context, success rate

**Inputs:**
- Runbook search results (0-3 runbooks)
- Manual solution search results (from troubleshooting history)
- Knowledge base references
- User context

**Outputs:**
- Unified ranked list (max 3 solutions)
- Primary recommendation (if confidence > 0.9)
- Presentation strategy (single vs multiple options)

**Ranking Algorithm:**
```
For each solution:
    base_score = semantic_similarity (0-1)

    context_bonus = 0
    IF server_type exact match: context_bonus += 0.1
    IF application exact match: context_bonus += 0.1
    IF OS exact match: context_bonus += 0.05
    IF environment exact match: context_bonus += 0.05

    success_bonus = historical_success_rate * 0.2

    automation_bonus = 0
    IF runbook (automated): automation_bonus += 0.15

    recency_bonus = 0
    IF used in last 30 days: recency_bonus += 0.05

    final_score = base_score + context_bonus + success_bonus +
                  automation_bonus + recency_bonus

    # Cap at 1.0
    final_score = min(final_score, 1.0)
```

**Prioritization Rules:**
1. Automated runbooks rank higher than manual (bonus +0.15)
2. High success rate solutions prioritized (up to +0.2)
3. Exact context matches get significant boost (+0.3 max)
4. Recently used solutions slightly favored (+0.05)

### 3. Response Generator

**Purpose:** Generate user-friendly response with multiple options

**Responsibilities:**
- Format solutions in unified structure
- Show clear trade-offs (time, risk, automation)
- Provide RBAC status for runbooks
- Include knowledge base references
- Generate recommendation explanation

**Template Structure:**
```
1. Primary Section (Runbook if available)
   - Title with automation indicator
   - Confidence stars (‚≠ê‚≠ê‚≠ê for >90%, ‚≠ê‚≠ê for 70-90%, ‚≠ê for <70%)
   - Estimated time
   - Success rate with history count
   - RBAC status
   - Action buttons

2. Alternatives Section (Manual Solutions)
   - Ranked list (max 2 alternatives)
   - Same metadata format
   - Show detailed steps on expand
   - Copy-friendly commands

3. Knowledge Base Section
   - Related documentation links
   - Design best practices
   - Tutorial references

4. Footer
   - Recommendation explanation
   - Warnings/prerequisites
   - Feedback buttons (üëç üëé)
```

### 4. Feedback Loop Integration

**Purpose:** Track which solutions users choose and learn preferences

**Data Collected:**
```
TroubleshootingSolutionFeedback:
  - session_id
  - query_id
  - solutions_presented [array of solution IDs]
  - solution_chosen (which one user selected)
  - execution_attempted (boolean)
  - execution_result (success/failed/abandoned)
  - time_to_decision (seconds until user chose)
  - user_feedback (thumbs up/down)
  - feedback_comment (optional text)
```

**Learning Loop:**
```
1. User selects Solution A from [A, B, C]
   ‚Üì
2. Record selection in feedback table
   ‚Üì
3. Update confidence scores:
   - Solution A: confidence += 0.05 (chosen)
   - Solution B: confidence -= 0.02 (not chosen)
   - Solution C: confidence -= 0.02 (not chosen)
   ‚Üì
4. Track user preferences:
   - Does user prefer automated (runbooks)?
   - Does user prefer quick fixes vs thorough investigation?
   - Does user prefer specific tools/approaches?
   ‚Üì
5. Personalization (future phase):
   - Rank solutions based on user history
   - Learn team/organization preferences
```

---

## Data Model Extensions

### New Tables Required

#### 1. Runbook Embeddings
```
runbook_embeddings:
  - runbook_id (UUID, FK to runbooks table)
  - embedding (vector(1536))
  - metadata (JSONB):
      - server_types []
      - applications []
      - os_compatibility []
      - environments []
  - success_rate (Numeric)
  - total_executions (Integer)
  - last_execution_at (Timestamp)
  - avg_duration_seconds (Integer)
  - created_at (Timestamp)
  - updated_at (Timestamp)

Indexes:
  - vector index on embedding (ivfflat or hnsw)
  - gin index on metadata
  - index on runbook_id
```

#### 2. Troubleshooting Solution Feedback
```
troubleshooting_solution_feedback:
  - id (UUID, PK)
  - session_id (UUID, FK to ai_helper_sessions)
  - user_id (UUID, FK to users)
  - query_id (UUID, FK to ai_helper_audit_logs)

  # Solutions presented
  - solutions_presented (JSONB array):
      [
        {
          "solution_id": "uuid",
          "type": "runbook|manual|knowledge",
          "confidence": 0.95,
          "rank": 1
        },
        ...
      ]

  # User action
  - solution_chosen_id (UUID)
  - solution_chosen_type (String: runbook|manual)
  - solution_chosen_rank (Integer: which rank user chose)
  - time_to_decision_seconds (Integer)

  # Execution tracking
  - execution_attempted (Boolean)
  - execution_started_at (Timestamp)
  - execution_completed_at (Timestamp)
  - execution_result (String: success|failed|abandoned|timeout)
  - execution_details (JSONB)

  # Feedback
  - user_feedback (String: helpful|not_helpful|partially_helpful)
  - user_feedback_comment (Text)
  - feedback_timestamp (Timestamp)

  # Context
  - page_context (JSONB)
  - server_context (JSONB)

  - created_at (Timestamp)

Indexes:
  - index on user_id
  - index on session_id
  - index on solution_chosen_type
  - index on execution_result
  - index on user_feedback
  - gin index on solutions_presented
```

#### 3. Update Existing Tables

**runbooks table** (if needed):
- Add embedding generation trigger
- Add success_rate tracking
- Add AI metadata (context tags)

**ai_helper_audit_logs table**:
- Add `solutions_presented` (JSONB array)
- Add `solution_chosen_id` (UUID)
- Links to troubleshooting_solution_feedback

---

## Integration Points

### 1. With Existing Knowledge Base
- Parallel search: Runbooks + Troubleshooting History + Knowledge Base
- Knowledge base provides design context and best practices
- Referenced as "Related Documentation" in response

### 2. With Existing Runbook System
- Runbook discovery via semantic search
- RBAC/ACL enforcement before showing runbook options
- Execution through existing runbook engine
- Success/failure tracking feeds back to confidence scores

### 3. With AI Helper Orchestrator
```python
# Enhanced workflow in ai_helper_orchestrator.py

async def handle_troubleshooting_query(query, context):
    # 1. Parallel search
    runbooks_task = search_runbooks(query, context)
    manual_solutions_task = search_troubleshooting_history(query, context)
    knowledge_task = search_knowledge_base(query, context)

    runbooks, manual_solutions, knowledge = await asyncio.gather(
        runbooks_task,
        manual_solutions_task,
        knowledge_task
    )

    # 2. Rank and combine
    ranked_solutions = rank_and_combine_solutions(
        runbooks=runbooks,
        manual=manual_solutions,
        knowledge=knowledge,
        user_context=context
    )

    # 3. Apply decision matrix
    presentation_strategy = apply_decision_matrix(ranked_solutions)

    # 4. Generate response
    response = generate_unified_response(
        solutions=ranked_solutions,
        strategy=presentation_strategy,
        knowledge_refs=knowledge
    )

    # 5. Log for feedback tracking
    log_solutions_presented(
        query_id=query.id,
        solutions=ranked_solutions
    )

    return response
```

### 4. With RBAC System
- Check runbook execution permissions before presenting
- Mark runbooks user cannot execute with üîí icon
- Filter out inaccessible runbooks from results
- Log RBAC denials for security audit

---

## User Experience Flow

### Scenario 1: High-Confidence Runbook Available

```
User: "High CPU on Apache server prod-web-01"

AI Response:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üéØ RECOMMENDED SOLUTION              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚úÖ Runbook #45: "Apache CPU Fix"    ‚îÇ
‚îÇ    ‚≠ê‚≠ê‚≠ê 95% confidence             ‚îÇ
‚îÇ    [‚ñ∂Ô∏è Execute] [üìñ Details]         ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ [See 2 alternative solutions]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

User clicks: [‚ñ∂Ô∏è Execute]
‚Üí Runbook executes with RBAC checks
‚Üí User gets real-time progress
‚Üí Success/failure logged
‚Üí Feedback prompt: "Was this helpful?"
```

### Scenario 2: Multiple Similar Solutions

```
User: "MySQL slow queries on db-master"

AI Response:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üéØ MULTIPLE SOLUTIONS AVAILABLE           ‚îÇ
‚îÇ (Confidence scores are similar)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. ‚úÖ Runbook: "MySQL Query Cache Tune"  ‚îÇ
‚îÇ    ‚≠ê‚≠ê‚≠ê 88% confidence                  ‚îÇ
‚îÇ    [‚ñ∂Ô∏è Execute]                           ‚îÇ
‚îÇ                                           ‚îÇ
‚îÇ 2. üîß Manual: "Index Optimization"       ‚îÇ
‚îÇ    ‚≠ê‚≠ê 85% confidence                    ‚îÇ
‚îÇ    [üìã Show Steps]                        ‚îÇ
‚îÇ                                           ‚îÇ
‚îÇ 3. üîß Manual: "Restart MySQL"            ‚îÇ
‚îÇ    ‚≠ê‚≠ê 82% confidence                    ‚îÇ
‚îÇ    [üìã Show Steps]                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üí° All solutions have worked in similar contexts.
   Choose based on your timeline and risk tolerance.

User chooses Option 2 (Manual Index Optimization)
‚Üí System records: User preferred manual over runbook
‚Üí Learn: This user likes investigative approach
‚Üí Future queries: Slightly favor manual solutions for this user
```

### Scenario 3: No High-Confidence Match

```
User: "Strange network latency spikes"

AI Response:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ö†Ô∏è  EXPERIMENTAL SOLUTIONS                ‚îÇ
‚îÇ (No high-confidence match found)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. üîß Check Network Interface Stats      ‚îÇ
‚îÇ    ‚≠ê 65% confidence                      ‚îÇ
‚îÇ    [üìã Show Commands]                     ‚îÇ
‚îÇ                                           ‚îÇ
‚îÇ 2. üîß Review Recent Network Changes      ‚îÇ
‚îÇ    ‚≠ê 62% confidence                      ‚îÇ
‚îÇ    [üìã Show Steps]                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üí° These solutions worked in loosely similar contexts.
   Please provide feedback after trying.

üìö Related docs: Network Troubleshooting Guide

User tries Option 1 and it works
‚Üí Provides feedback: üëç "Helpful"
‚Üí System learns: This solution works for network latency
‚Üí Confidence increases to 75% for next time
‚Üí Creates troubleshooting case for future reference
```

---

## RBAC and Security Considerations

### Runbook Execution Permissions

**Pre-Execution Checks:**
1. User has `runbook.execute` permission globally
2. User has permission for specific runbook (ACL)
3. User has permission for target servers (server RBAC)
4. Runbook status is `active` (not disabled)
5. User's role allows execution in target environment (prod/staging)

**Response Handling:**
```
IF user has full permissions:
   ‚Üí Show runbook with [‚ñ∂Ô∏è Execute] button
   ‚Üí Mark as "You can execute this"

ELSE IF user has view permission only:
   ‚Üí Show runbook details (read-only)
   ‚Üí Mark as "üîí Execution requires elevated permissions"
   ‚Üí Show who can execute (roles)

ELSE (no permission):
   ‚Üí Don't show runbook in results
   ‚Üí Only show manual alternatives
```

### Audit Logging

**What to Log:**
- Solutions presented (runbook + manual options)
- Which solution user chose
- RBAC checks performed
- Execution attempts (success/failure)
- User feedback
- Time to decision

**Security Events:**
- User attempted runbook without permission
- User modified suggested commands before execution
- Execution blocked by RBAC
- Suspicious patterns (rapid executions, unusual targets)

---

## Performance Considerations

### Search Latency
- **Target:** < 500ms for parallel search (runbooks + manual + knowledge)
- **Optimization:**
  - Use pgvector indexes (ivfflat or hnsw)
  - Cache frequently-searched runbooks
  - Pre-filter by context before semantic search
  - Limit search to top 10 candidates per source

### Ranking Latency
- **Target:** < 100ms for ranking and combining
- **Optimization:**
  - Pre-compute success rates (updated async)
  - Cache user preference profiles
  - Simple weighted scoring (avoid complex ML)

### Total Response Time
- **Target:** < 1 second end-to-end
- **Breakdown:**
  - Parallel search: 500ms
  - Ranking: 100ms
  - Response generation: 200ms
  - LLM formatting (if needed): 200ms

---

## Metrics and Monitoring

### Success Metrics

1. **Solution Acceptance Rate**
   - % of presented solutions that users choose to execute
   - Target: > 70%

2. **Runbook Utilization**
   - % of queries where runbook was available and chosen
   - Target: > 50% when runbook matches exist

3. **First-Choice Accuracy**
   - % of times user chooses top-ranked solution
   - Target: > 60%

4. **User Satisfaction**
   - Thumbs up rate after solution execution
   - Target: > 80%

5. **Execution Success Rate**
   - % of chosen solutions that successfully resolve issue
   - Target: > 85%

### Monitoring Dashboards

**Dashboard 1: Solution Quality**
- Average confidence scores over time
- Confidence vs actual success correlation
- Low-confidence solution performance

**Dashboard 2: Runbook vs Manual**
- Runbook availability rate
- Runbook choice rate (when available)
- Runbook success rate vs manual success rate

**Dashboard 3: User Behavior**
- Time to decision (how long users take to choose)
- Option rank distribution (do users always choose #1?)
- Modification rate (how often users modify manual commands)

**Dashboard 4: Learning Effectiveness**
- Confidence score improvements over time
- New solution discovery rate
- Feedback incorporation lag

---

## Implementation Phases (High-Level)

### Phase 1: Foundation (Runbook Search)
**Goal:** Enable runbook discovery via semantic search

**Components:**
- Runbook embedding generation
- Runbook search service
- RBAC integration
- Basic response formatting

**Success Criteria:**
- Runbooks searchable via natural language
- RBAC-filtered results
- < 500ms search latency

### Phase 2: Solution Ranking
**Goal:** Combine runbooks + manual solutions with intelligent ranking

**Components:**
- Solution ranker implementation
- Decision matrix logic
- Unified response generator
- Context-aware filtering

**Success Criteria:**
- Multiple solutions presented when appropriate
- Ranking accuracy > 60% (user chooses top option)
- Response generation < 100ms

### Phase 3: Feedback Loop
**Goal:** Learn from user choices to improve recommendations

**Components:**
- Feedback data collection
- Confidence score updates
- User preference tracking
- A/B testing framework

**Success Criteria:**
- Feedback captured for > 80% of interactions
- Confidence scores improve over 30 days
- User satisfaction > 80%

### Phase 4: Personalization (Future)
**Goal:** Tailor recommendations to individual users and teams

**Components:**
- User preference models
- Team/organization patterns
- Contextual ranking adjustments
- Proactive runbook suggestions

**Success Criteria:**
- Personalized ranking improves accuracy by 10%
- User satisfaction > 85%
- Time to decision decreases by 20%

---

## Open Questions and Future Considerations

### Open Questions
1. **Runbook suggestion frequency:** How often should we suggest creating a runbook from frequently-used manual solutions?
2. **Cross-solution learning:** If a manual solution works well, should we suggest automating it as a runbook?
3. **Fallback strategy:** What if neither runbooks nor manual solutions have high confidence? Should we engage expert humans?
4. **Multi-step solutions:** How to handle solutions that require multiple stages (diagnostic ‚Üí action)?

### Future Enhancements
1. **Proactive suggestions:** AI monitors metrics and suggests solutions before user asks
2. **Solution chaining:** Combine multiple runbooks/commands into a workflow
3. **Confidence explanation:** Show why AI is confident (similar cases, success rate breakdown)
4. **Solution evolution tracking:** Track how solutions change over time as systems evolve
5. **Collaborative filtering:** Learn from similar users' choices (Netflix-style recommendations)

---

## Risk and Mitigation

### Risk 1: Low Runbook Adoption
**Risk:** Users might always choose manual solutions over runbooks

**Mitigation:**
- Track runbook vs manual choice rate
- Identify why users prefer manual (time, trust, flexibility)
- Improve runbook descriptions and success rate visibility
- Show time savings with runbook execution
- Gamification: Badge for "automation advocate"

### Risk 2: RBAC Complexity
**Risk:** Permission checking might slow down response or create false negatives

**Mitigation:**
- Cache user permissions (refresh every 5 minutes)
- Pre-filter runbooks by user role before semantic search
- Log RBAC denials for security team review
- Provide clear messaging when runbooks are hidden due to permissions

### Risk 3: Poor Ranking Quality
**Risk:** Users might frequently choose lower-ranked options, indicating bad ranking

**Mitigation:**
- A/B test ranking algorithms
- Track "rank inversion rate" (user chooses #3 when #1 was available)
- Manual review of low-confidence cases
- Incorporate human expert feedback into ranking

### Risk 4: Feedback Fatigue
**Risk:** Users might ignore feedback prompts, reducing learning signal

**Mitigation:**
- Only prompt for feedback on critical paths
- Use implicit feedback (which solution chosen) as primary signal
- Make feedback optional and quick (thumbs up/down)
- Show impact: "Your feedback helped 12 other users"

### Risk 5: Context Mismatch
**Risk:** Solutions might be recommended for wrong context (wrong OS, app version, etc.)

**Mitigation:**
- Strict context filtering before search
- Require minimum context match threshold (> 70%)
- Show context details in solution presentation
- Allow users to report "wrong context" as feedback
- Tag solutions with required context attributes

---

## Success Criteria

### Must Have (P0)
- ‚úÖ Present multiple solutions when confidence scores are similar (< 0.1 difference)
- ‚úÖ Runbook-first search and prioritization
- ‚úÖ RBAC/ACL enforcement for runbook execution
- ‚úÖ Track which solution users choose (feedback loop)
- ‚úÖ Context-aware filtering (server type, app, OS, environment)

### Should Have (P1)
- Confidence score improvements over time (30-day measurement)
- User satisfaction > 80% (thumbs up rate)
- First-choice accuracy > 60% (users choose top option)
- Response latency < 1 second end-to-end

### Nice to Have (P2)
- User preference personalization
- Proactive runbook creation suggestions
- Cross-team learning (organization-wide patterns)
- Confidence explanation ("Why am I seeing this solution?")

---

## Conclusion

This high-level design transforms AI Helper from a single-suggestion system to an intelligent, multi-option recommender that:

1. **Prioritizes automation** via runbook-first search
2. **Respects user choice** by presenting multiple ranked options
3. **Learns continuously** from user selections and feedback
4. **Enforces security** through RBAC/ACL integration
5. **Adapts to context** using multi-dimensional filtering

The design is built on proven patterns:
- Semantic search (pgvector) for runbook and solution discovery
- Hybrid feedback (explicit + implicit) for continuous learning
- Weighted ranking combining confidence, context, and historical success
- User-centric presentation with clear trade-offs

**Next Steps:**
1. Review and approve this high-level design
2. Detail database schema for new tables
3. Design API contracts for new services
4. Create detailed implementation plan for Phase 1

---

**Document Status:** Ready for Review
**Approvers:** Product Owner, Engineering Lead, Security Team
**Related Documents:**
- AI_LEARNING_PLAN.md (Overall AI learning strategy)
- AI_AGENT_GRAFANA_BUILDER_MODE_FIX.md (Grafana integration)
- Existing runbook system documentation
- RBAC/ACL system documentation
